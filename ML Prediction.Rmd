---
title: "ML Prediction Assignment"
output: html_document
author: D. Ansell
date: "2025-04-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( message = FALSE
                      ,warning = FALSE)
```

## Introduction

The goal of this project is to predict the manner in which an exercise was performed; i.e. the "classe" variable in the training set described here: <br>
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har
<br>
Various machine learning models with be evaluated and selected as a predictor.

## Data Sourcing & Processing
Training data was downloaded into the working directory from this url:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
<P>
Testing data was downloaded into the working directory from this url:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

No processing was done on the files until after being loaded into R (v4.4.1). The training dataset was inspected and found to have three timestamp columns, which were removed. It was then analyzed to determine if any columns had a high number of NA values. If a column's observations were more than 95% NA, that column was deemed to be potentially problematic to the prediction models and was removed.
```{r include=TRUE}
library(readr)
library(caret)
library(parallel)
library(doParallel)
library(ggplot2)
library(printr)
```

```{r include=TRUE}
set.seed(1984)    #random seed set for reproducibility

pml_testing = read_csv("pml-testing.csv")
pml_training = read_csv("pml-training.csv")

# Remove Timestamp columns
pml_training2 = pml_training[, !grepl("timestamp", names(pml_training))]

# Check and remove columns with more than 95% <NA>.
na_rate = colMeans(is.na(pml_training2))
high_na_cols = which(na_rate >= .95)
pml_training3 = pml_training2[, -high_na_cols]
```

Since the pml_testing dataset does not have a *classe* column (i.e. the outcome variable), 20% of the training dataset was split off into a validation dataset to be used to quantatively measure the out-of-sample error.
```{r}
# Separate into train and validate sets
in_train = createDataPartition(pml_training3$classe, p=0.8, list=FALSE)
validate = pml_training3[-in_train, ]
train = pml_training3[in_train, ]

dim_validate = dim(validate)
dim_train = dim(train)
```

The [rows, columns] count of the clean training dataset is `r dim_train`.


## Model Building

**Leave-One-Out Cross-Validation (LOOCV)** is not suitable, as it is intended for a very low number of observations. As well, the computational costs would be high given that the training set contains 15699 observations and an identical number of models would be built under LOOCV. LOOCV would also produce overfit models with high variance.

**5-fold Cross-Validation** was chosen for the training, as implemented by the Caret package.
<br>
&nbsp;<p/>
#### Choice of Classification Model
**Support Vector Machines:** SVM Handles complex decision boundaries, but SVM does not scale well when the dataset is large. SVM was not selected as a candidate model. 
<p>
**Random Forest:** RF models typically return high accuracy for classification tasks and need no preprocessing under Caret. A rf model was built against the training data.
<p>
**Gradient Boosting Machines:** GBM is an ideal choice for classifying high-dimensional data, can perform automatic feature selection, and can identify complex, non-linear relationships between predictors. a gbm model was built against the training data as well.
<p>

```{r include=TRUE}

cv_control = trainControl(method="cv", number=5, allowParallel = TRUE)
cluster = makePSOCKcluster(8) # 8 cores
registerDoParallel(cluster)
```

```{r rf , cache=TRUE, include=TRUE}
# Random Forest.
model_rf = train(classe ~ .,
                 data=train, 
                 method="rf", 
                 trControl=cv_control)
```

```{r figure_1, out.width="65%", fig.align="center", include=TRUE}
ggplot(model_rf) +
  labs(title="Accuracy of Random Forest Model",
       caption="Figure 1.") +
  theme_minimal()
```

```{r gbm, cache=TRUE, include=TRUE}
# gbm
# gbm doesn't automatically handle NA'S, so pre-process with knnImpute.
model_gbm = train(classe ~ .,
                  data=train, 
                  method="gbm", trControl=cv_control, 
                  preProcess=c("knnImpute"))
```

```{r figure_2, out.width="65%", fig.align="center", include=TRUE}
ggplot(model_gbm) +
  labs(title="Accuracy of Gradient Boosting Machine Model",
       caption="Figure 2.") +
  theme_minimal()
```

```{r include=FALSE}
# Clean up after parallel processing
stopCluster(cluster)
registerDoSEQ()
```

## Out of Sample Error

The validate dataset [`r dim_validate`] is used to measure the out-of-sample error.     
<p>
**Random Forest:** The rf model scored 100% accuracy (0.0% out-of-sample error rate) with a Kappa value of 1, meaning that there is perfect agreement between the predicted and actual classifications, beyond any agreement by chance:

```{r}
val_rf = predict(model_rf, validate)
confusionMatrix(factor(validate$classe), val_rf)
```
<p>
**GBM:** The gbm model scored 99.97% accuracy with a Kappa value of 0.9997. The out-of-sample error rate is 1 - .9997 = 0.0003. Only one observation was misclassified:
```{r}
val_gbm = predict(model_gbm, validate)
confusionMatrix(factor(validate$classe), val_gbm)
```

## Predictions

```{r}
predictions_rf = predict(model_rf, pml_testing)
predictions_gbm = predict(model_rf, pml_testing)
prob_id = as.character(pml_testing$problem_id)

predictions_table = t(data.frame(predictions_rf, predictions_gbm))
colnames(predictions_table) = prob_id
row.names(predictions_table) = c("Random Forest", "GBM")
predictions_table
```

